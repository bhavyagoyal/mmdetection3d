#!/usr/bin/env bash
#SBATCH --partition=research
#SBATCH --cpus-per-task=8
#SBATCH --gres=gpu:gtx1080:2
#SBATCH --mem=36G
#SBATCH --time=48:0:0
###SBATCH --nodelist=euler28
#SBATCH --exclude=euler04,euler05,euler24,euler25,euler26,euler27
###SBATCH --exclude=euler01,euler05
#SBATCH -o slurm.%j.%N.out # STDOUT
#SBATCH -e slurm.%j.%N.err # STDERR
#SBATCH --job-name=mmdt3d
###SBATCH --no-requeue

#conda 23.3.1, cuda11.8
#conda install pytorch==2.0.0 torchvision==0.15.0 torchaudio==2.0.0 pytorch-cuda=11.8 -c pytorch -c nvidia
#install mmcv without mim, using pip instead
#pip install "mmcv>=2.0.1" -f  https://download.openmmlab.com/mmcv/dist/cu118/torch2.0.0/index.html
#pip install mmdet==3.1.0
#git clone https://github.com/open-mmlab/mmdetection3d.git -b dev-1.x
## "-b dev-1.x" means checkout to the `dev-1.x` branch.
#cd mmdetection3d
#pip install -v -e .


module load anaconda/mini/23.3.1
module load nvidia/cuda/11.8.0
bootstrap_conda
conda activate openmmlab

SBR=$1
CHECKPOINT=checkpoints/votenet_16x8_sunrgbd-3d-10class_20210820_162823-bf11f014.pth
#EXPERIMENT=baselines/gtdepth/points20000_wofeats
#EXPERIMENT=work_dir_py/1_50_filteringpeaks_points50000+densitynormed+weighted+fps5000/votenet_8xb16_sunrgbd-3d/
#EXPERIMENT=work_dir_py/cf-wsp/1_50/updated/fps5000_sp01_post_featsp_attnmask
#EXPERIMENT=work_dir_py/cf-wsp/5_100/updated/sp01_post_updatedfps05/
#EXPERIMENT=work_dir_py/cf-wsp/5_100/rerun/spupdated01_post_updatedfps05/
#EXPERIMENT=work_dir_py/cf-wsp/all4/updated/sp01_post_updatedfps04_e12/
#EXPERIMENT=work_dir_py/compression/32/cf-wsp/5_50/sp01_post_updatedfps05
#EXPERIMENT=work_dir_py/baselines/argmax_filtering_allprob1/1_50/sp03_testing
EXPERIMENT=work_dir_py/npeaks/rerun/1peak/1_50/first50000_spupdated03_post_updatedfps05_nspobjweightedeval
EXPERIMENT=work_dir_py/npeaks/min2/1peak/1_50/first50000_spupdated0_cfupdated04_post_newfpscf05
EXPERIMENT=work_dir_py/sbr/0.3/all5/first50000_spupdated000001_post_finfps003
EXPERIMENT=work_dir_py/gaussian_noise/score-denoised/0.1/first20000
#EXPERIMENT=work_dir_py/sbr/1.0/score-denoised_sunrgbd1000/5_50_large/first10000
#EXPERIMENT=work_dir_py/sbr/0.3/all5/first10000
#50000_spupdated0002_post_newfps0003
#updatedfps05_newfps_wofpsshuffle
#EXPERIMENT=work_dir_py/baselines/argmax-filtering/min2/all5/first10000
#first50000_spupdated01_post_updatedfps07
#fps20482048512256
#EXPERIMENT=work_dir_py/compression/32/argmax/5_50/points2048
#EXPERIMENT=work_dir_py/baselines/3dcnndenoise-argmax/denoise/5_50/points20000
CHECKPOINT=${EXPERIMENT}/epoch_36.pth
#DATAPATH=py/points_5_5_gtdepth
#DATAPATH=wsp_cf/peaks-confidence/5_100
#DATAPATH=points_baseline/argmax-filtering-conf/1_50/
DATAPATH=points_cfwsp/1/peaks-confidence/1_50/
DATAPATH=points_min2/0.3/argmax-filtering-sbr/
DATAPATH=points_gaussian/score-denoised/0.1/pcl/
#DATAPATH=points_min2/1.0/score-denoised_sunrgbd1000/argmax-filtering-sbr/5_50_large/pcl/

#DATAPATH=points_min2/0.3/argmax-filtering-sbr/
#DATAPATH=points_min2/argmax-filtering-conf/
#DATAPATH=points_compression/32/decompressed-argmax/5_50/
#DATAPATH=points_compression/32/decompressed-peaks-confidence/1_50/
#argmax-filtering-conf/
#DATAPATH=points_baseline/3dcnndenoise-argmax/denoise/5_50/
GPUS=2
PORTUSED=$(( $RANDOM + 10000 ))


## Cache files in /tmp if io is slow over network mount disk
## works but not really helpful if point clouds .bin files are big
#TMPFOLDER=/scratch/sunrgbd/
#TMPDATA=${TMPFOLDER}${DATAPATH}
#USECACHE=true
#if [ "$USECACHE" = true ]; then
#mkdir -p $TMPFOLDER
##cp --parent data/sunrgbd/$DATAPATH/00{6,7,8,9}*.bin ${TMPFOLDER} &
#fi


#CUDA_VISIBLE_DEVICES=0 python -u tools/train.py configs/votenet/votenet_8xb16_sunrgbd-3d.py --auto-scale-lr --resume --cfg-options \
PORT=${PORTUSED} ./tools/dist_train.sh configs/votenet/votenet_8xb16_sunrgbd-3d.py ${GPUS} --auto-scale-lr --resume --cfg-options \
	train_dataloader.dataset.dataset.data_prefix.pts=${DATAPATH} \
	val_dataloader.dataset.data_prefix.pts=${DATAPATH} \
	train_dataloader.batch_size=8 \
	work_dir=${EXPERIMENT} \
	default_hooks.checkpoint.interval=1 \
	train_dataloader.dataset.dataset.pipeline.4.num_points=20000 \
	val_dataloader.dataset.pipeline.1.transforms.2.num_points=20000 \
	train_dataloader.dataset.dataset.pipeline.4.firstk_sampling=True \
	val_dataloader.dataset.pipeline.1.transforms.2.firstk_sampling=True \
	train_dataloader.dataset.dataset.pipeline.0.load_dim=8 \
	val_dataloader.dataset.pipeline.0.load_dim=8 \
	train_dataloader.dataset.dataset.pipeline.0.use_dim="[0,1,2]" \
	val_dataloader.dataset.pipeline.0.use_dim="[0,1,2]" \





	#model.neighbor_score=0.3 \
	#model.filter_index=4 \




	#train_dataloader.dataset.dataset.ann_file='sunrgbd_infos_train_1_100_1_50_5_100_5_50_clean.pkl' \
	#val_dataloader.dataset.ann_file='sunrgbd_infos_val_1_100_1_50_5_100_5_50_clean.pkl' \
	#param_scheduler.0.end=12 \
	#param_scheduler.0.milestones=[8,10] \
	#train_cfg.max_epochs=12 \
	#model.post_sort=4 \
	#model.updated_fps=0.03 \




	#val_dataloader.dataset.pipeline.1.transforms.2.thresh_sampling=0.2 \
	#train_dataloader.dataset.dataset.pipeline.4.thresh_sampling=0.2 \
	#model.new_fps_strat=True \
	#train_dataloader.dataset.dataset.pipeline.0.unit_probabilities=True \
	#val_dataloader.dataset.pipeline.0.unit_probabilities=True \


	#model.shuffle_stack=True \
	#train_dataloader.num_workers=4 \
	#model.backbone.in_channels=4 \
















	#train_dataloader.dataset.dataset.pipeline.4.topk_sampling=True \

	#model.bbox_head.proposals_conf=1 \
	#model.bbox_head.clip=0.9 \
	#model.train_cfg.sample_mode="random" \ 
	#model.test_cfg.sample_mode="random" \ 
	#train_dataloader.dataset.dataset.pipeline.0.norm_probabilities=True \
	#val_dataloader.dataset.pipeline.0.norm_probabilities=True \

	#train_dataloader.dataset.dataset.pipeline.4.pre_sort=None \
	#val_dataloader.dataset.pipeline.1.transforms.2.pre_sort=None \
	#train_dataloader.dataset.dataset.pipeline.0.unit_probabilities=True \
	#val_dataloader.dataset.pipeline.0.unit_probabilities=True \
	#model.max_neighbor=1024 \
	#val_dataloader.dataset.pipeline.1.transforms.2.probability_sampling=True \
	#train_dataloader.dataset.dataset.pipeline.4.probability_sampling=True \
	#train_dataloader.dataset.dataset.pipeline.0.cache_prefix="${TMPFOLDER}" \
#	model.backbone.sa_cfg.pool_mod='avgmax' \
	#train_dataloader.sampler.shuffle=False \
	#model.backbone.fps_sample_range_list="[5000,-1,-1,-1]" \
	#val_dataloader.dataset.pipeline.1.transforms.2.topk_sampling=True \
	#model.backbone.sa_mask=True \
	#model.backbone.num_points="[2048,2048,512,256]" \
	#model.weighted_filtering_score=True \


#if [ "$USECACHE" = true ]; then
#rm -rf ${TMPDATA}
#fi
 
